{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOW3QX4R7NPfDrWElYMWsTl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrDys/stable-diffusion-pytorch-to-flax/blob/main/Stable_Diffusion_PyTorch_to_Flax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stable Diffusion PyTorch to Flax\n",
        "This notebook will convert a PyTorch-formatted Stable Diffusion model to a Flax model, optionally in `bfloat16` format, for use with TPUs.\n",
        "\n",
        "This notebook should be run with a GPU runtime. Check your runtime type by going to **Runtime â®• Change Runtime Type**. If, during conversion, you encounter memory errors (likely, to be honest), change your **Runtime Shape** (from the same menu) to be **High-RAM**.\n",
        "\n",
        "Notebook by [Sean Hannan](https://www.seaphantasm.com). The key to figuring a lot of this out was buried in [a GitHub issue comment](https://github.com/huggingface/diffusers/issues/1015#issuecomment-1297504201)."
      ],
      "metadata": {
        "id": "5P3uSALqSIik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Connect to Google Drive (optional)\n",
        "#@markdown If your model is on Google Drive or you wish the save the output to Google Drive, run this cell to connect.\n",
        "from google.colab import drive # type: ignore\n",
        "try:\n",
        "    drive_path = \"/content/drive\" #@param {type:\"string\"}\n",
        "    drive.mount(drive_path,force_remount=True)\n",
        "except:\n",
        "    print(\"Error mounting drive.\\n\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "OH3fkP-wS0Xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qSAkLcWZRyjy"
      },
      "outputs": [],
      "source": [
        "#@title Connect to Hugging Face (optional)\n",
        "#@markdown If you want to download a model from [Hugging Face](https://huggingface.co) as part of the conversion, run this cell. It will prompt you for an API token. Make sure you you have agreed to any license agreement on the model card, or you may run into errors.\n",
        "from IPython.display import clear_output, display\n",
        "!pip install huggingface_hub==0.10.0 gradio\n",
        "clear_output()\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "!git config --global credential.helper store\n",
        "\n",
        "notebook_login()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Convert Model\n",
        "#@markdown Set the location where the model will be saved.\n",
        "output_path = \"/content/drive/MyDrive/output/path\" #@param {type:\"string\"}\n",
        "import os\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "#@markdown Select the format of your model. `bfloat16` (or `bf16`) is a half-precision format ideal for running on TPUs. \n",
        "format = \"bfloat16\" #@param [\"bfloat16\", \"float32\"]\n",
        "\n",
        "#@markdown The model to convert. It can be a path to a model on Google Drive or it can be on Hugging Face in the form of \"&lt;account&gt;/&lt;model name&gt;\".\n",
        "model = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown If the model above is a path to a checkpoint (.ckpt) file, we first need to convert it to diffusers format.\n",
        "is_checkpoint = False #@param {type:\"boolean\"}\n",
        "\n",
        "!pip install --upgrade jax jaxlib\n",
        "!pip install -U jax[cuda11_cudnn82] -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
        "!pip install flax diffusers transformers ftfy\n",
        "\n",
        "if is_checkpoint:\n",
        "    !pip install OmegaConf\n",
        "    ![ ! -d \"/content/diffusers\" ] && git clone https://github.com/huggingface/diffusers.git /content/diffusers\n",
        "    !python /content/diffusers/scripts/convert_original_stable_diffusion_to_diffusers.py --checkpoint_path=$model --dump_path=/content/diffconversion\n",
        "    model = \"/content/diffconversion\"\n",
        "\n",
        "# Adapted from huggingface's transformers library\n",
        "# https://github.com/huggingface/transformers/blob/b9a0ede6ab2558197d919e7a77a96dfd1c466b3f/src/transformers/modeling_flax_utils.py#L294-L355\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from typing import Dict, Union\n",
        "from flax.core.frozen_dict import FrozenDict\n",
        "from diffusers import FlaxStableDiffusionPipeline\n",
        "\n",
        "def to_bf16(params: Union[Dict, FrozenDict]):\n",
        "    def conditional_cast(param):\n",
        "        if isinstance(param, jnp.ndarray) and jnp.issubdtype(param.dtype, jnp.floating):\n",
        "            param = param.astype(jnp.bfloat16)\n",
        "        return param\n",
        "\n",
        "    return jax.tree_util.tree_map(conditional_cast, params)\n",
        "\n",
        "pipeline, params = FlaxStableDiffusionPipeline.from_pretrained(model, from_pt=True)\n",
        "if format == \"bfloat16\":\n",
        "  params = to_bf16(params=params)\n",
        "\n",
        "pipeline.save_pretrained(output_path, params=params)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "47PIdoIFXYl_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}